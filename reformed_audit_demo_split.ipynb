{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da730348",
   "metadata": {},
   "source": [
    "# Reformed-Audit Demo (Split Cells, ROW_CAP=10k)\n",
    "This notebook is generated to help you debug cell-by-cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9de9e",
   "metadata": {},
   "source": [
    "### 1) Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a0ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_PATH: train.csv\n",
      "ART_DIR: .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json, math, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = \"train.csv\" if os.path.exists(\"train.csv\") else \"train.csv\"\n",
    "ART_DIR = \"data\" if os.path.exists(\"data\") else \".\"\n",
    "ROW_CAP = 10000\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "print(\"CSV_PATH:\", CSV_PATH)\n",
    "print(\"ART_DIR:\", ART_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97535c1d",
   "metadata": {},
   "source": [
    "### 2) Load Data & Composite Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538c830b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Missing file: train.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(CSV_PATH), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m df = pd.read_csv(CSV_PATH)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) > ROW_CAP:\n",
      "\u001b[31mAssertionError\u001b[39m: Missing file: train.csv"
     ]
    }
   ],
   "source": [
    "\n",
    "assert os.path.exists(CSV_PATH), f\"Missing file: {CSV_PATH}\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if len(df) > ROW_CAP:\n",
    "    df = df.sample(ROW_CAP, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Capped rows to {ROW_CAP}.\")\n",
    "\n",
    "prompt_col = \"prompt\" if \"prompt\" in df.columns else None\n",
    "text_col   = \"response\" if \"response\" in df.columns else None\n",
    "\n",
    "subscores_order = [\"helpfulness\",\"correctness\",\"coherence\",\"complexity\",\"verbosity\"]\n",
    "subs_present = [c for c in subscores_order if c in df.columns]\n",
    "score_like = [c for c in df.columns if c.lower() in {\"score\",\"rating\",\"label\",\"quality\"}]\n",
    "\n",
    "if not score_like and subs_present:\n",
    "    weights = {\"helpfulness\":0.35,\"correctness\":0.30,\"coherence\":0.25,\"complexity\":0.05,\"verbosity\":0.05}\n",
    "    w = np.array([weights.get(c, 0.05) for c in subs_present], dtype=float); w = w / w.sum()\n",
    "    df[\"score\"] = (df[subs_present] * w).sum(axis=1)\n",
    "    score_col = \"score\"\n",
    "else:\n",
    "    score_col = score_like[0] if score_like else None\n",
    "\n",
    "assert text_col is not None and score_col is not None, f\"Need text & score. Found text={text_col}, score={score_col}\"\n",
    "keep = [c for c in [prompt_col, text_col, score_col] if c]\n",
    "df = df[keep].dropna().rename(columns={text_col:\"text\", (prompt_col or \"prompt\"):\"prompt\", score_col:\"score\"})\n",
    "print(\"Detected:\", {\"text\":\"response\",\"prompt\":prompt_col,\"score\":score_col})\n",
    "print(\"Rows:\", len(df))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230618a5",
   "metadata": {},
   "source": [
    "### 3) Labels & Baseline Model (TF-IDF + LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560912f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m high_thr = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m].quantile(\u001b[32m0.75\u001b[39m)\n\u001b[32m      2\u001b[39m low_thr  = df[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m].quantile(\u001b[32m0.25\u001b[39m)\n\u001b[32m      3\u001b[39m df_bin = df[(df[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m] >= high_thr) | (df[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m] <= low_thr)].copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "high_thr = df[\"score\"].quantile(0.75)\n",
    "low_thr  = df[\"score\"].quantile(0.25)\n",
    "df_bin = df[(df[\"score\"] >= high_thr) | (df[\"score\"] <= low_thr)].copy()\n",
    "df_bin[\"y\"] = (df_bin[\"score\"] >= df_bin[\"score\"].median()).astype(int)\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\", min_df=5, max_df=0.9, max_features=5000)\n",
    "X = tfidf.fit_transform(df_bin[\"text\"].astype(str))\n",
    "y = df_bin[\"y\"].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te, raw_tr, raw_te = train_test_split(\n",
    "    X, y, df_bin[\"text\"].astype(str), test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "clf.fit(X_tr, y_tr)\n",
    "\n",
    "proba = clf.predict_proba(X_te)[:,1]\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_te, pred)),\n",
    "    \"f1\": float(f1_score(y_te, pred)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_te, proba)),\n",
    "    \"n_test\": int(len(y_te)),\n",
    "    \"n_rows_used\": int(len(df_bin))\n",
    "}\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e4945",
   "metadata": {},
   "source": [
    "### 4) Top Words (High vs Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = np.array(tfidf.get_feature_names_out())\n",
    "coefs = clf.coef_.ravel()\n",
    "top_pos_idx = np.argsort(coefs)[-25:][::-1]\n",
    "top_neg_idx = np.argsort(coefs)[:25]\n",
    "top_pos_words = [(vocab[i], float(coefs[i])) for i in top_pos_idx]\n",
    "top_neg_words = [(vocab[i], float(coefs[i])) for i in top_neg_idx]\n",
    "print(\"Top HIGH words:\", top_pos_words[:10])\n",
    "print(\"Top LOW words :\", top_neg_words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a8abc",
   "metadata": {},
   "source": [
    "### 5) Gaming Demo (append high-weight words + boilerplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5828329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HIGH_WORDS = [w for w,_ in top_pos_words[:12]] or [\"fair\",\"transparent\",\"privacy\",\"safety\",\"evidence\"]\n",
    "\n",
    "def game_texts(texts, high_words=HIGH_WORDS, min_tokens_add=30):\n",
    "    filler = \" \".join(high_words * max(1, int(np.ceil(min_tokens_add / max(1, len(high_words)))) ))\n",
    "    suffix = (\" In practice, we will carefully evaluate fairness, privacy, and transparency, \"\n",
    "              \"justify trade-offs, provide evidence, document limitations, and ensure accountability.\")\n",
    "    return [(t or \"\").strip() + \" \" + filler + \".\" + suffix for t in texts]\n",
    "\n",
    "lowest = np.argsort(proba)[: min(10, len(proba))]\n",
    "low_texts = raw_te.iloc[lowest].tolist()\n",
    "gamed_texts = game_texts(low_texts)\n",
    "\n",
    "X_low  = tfidf.transform(low_texts)\n",
    "X_game = tfidf.transform(gamed_texts)\n",
    "\n",
    "proba_low  = clf.predict_proba(X_low)[:,1]\n",
    "proba_game = clf.predict_proba(X_game)[:,1]\n",
    "\n",
    "gaming_df = pd.DataFrame({\"orig_proba_high\": proba_low, \"gamed_proba_high\": proba_game})\n",
    "gaming_df[\"delta\"] = gaming_df[\"gamed_proba_high\"] - gaming_df[\"orig_proba_high\"]\n",
    "gaming_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17978680",
   "metadata": {},
   "source": [
    "### 6) Fragility Check (strip top-K words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739740a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOPK = 50\n",
    "frag_set = set([w for w,_ in top_pos_words[:TOPK]] + [w for w,_ in top_neg_words[:TOPK]])\n",
    "\n",
    "def strip_topk_bulk(texts):\n",
    "    if not frag_set:\n",
    "        return texts\n",
    "    patt = re.compile(r\"\\b(\" + \"|\".join(re.escape(w) for w in frag_set) + r\")\\b\", flags=re.IGNORECASE)\n",
    "    return [patt.sub(\"\", t) for t in texts]\n",
    "\n",
    "X_te_stripped = tfidf.transform(strip_topk_bulk(raw_te.tolist()))\n",
    "proba_stripped = clf.predict_proba(X_te_stripped)[:,1]\n",
    "pred_stripped = (proba_stripped >= 0.5).astype(int)\n",
    "\n",
    "fragility_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_te, pred_stripped)),\n",
    "    \"f1\": float(f1_score(y_te, pred_stripped)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_te, proba_stripped))\n",
    "}\n",
    "fragility_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de681b23",
   "metadata": {},
   "source": [
    "### 7) Context Randomization (token shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffle_tokens(t: str) -> str:\n",
    "    tokens = re.findall(r\"\\w+|\\S\", t or \"\")\n",
    "    rng = np.random.default_rng(0)\n",
    "    rng.shuffle(tokens)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_te_shuffled = tfidf.transform([shuffle_tokens(t) for t in raw_te.tolist()])\n",
    "proba_shuf = clf.predict_proba(X_te_shuffled)[:,1]\n",
    "pred_shuf = (proba_shuf >= 0.5).astype(int)\n",
    "\n",
    "shuffle_metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_te, pred_shuf)),\n",
    "    \"f1\": float(f1_score(y_te, pred_shuf)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_te, proba_shuf))\n",
    "}\n",
    "shuffle_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c089f9",
   "metadata": {},
   "source": [
    "### 8) Prompt-Bucket Distribution Shift (fresh model/vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ff7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fresh TF-IDF/model so baseline vocab isn't mutated\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "if \"prompt\" in df_bin.columns and df_bin[\"prompt\"].notna().any():\n",
    "    def bucket_prompt(p: str) -> int:\n",
    "        p = p if isinstance(p, str) else str(p)\n",
    "        h = int(hashlib.md5(p.encode(\"utf-8\")).hexdigest(), 16)\n",
    "        return h % 4\n",
    "    df_pb = df_bin.copy()\n",
    "    df_pb[\"_pbkt\"] = df_pb[\"prompt\"].apply(bucket_prompt)\n",
    "\n",
    "    tr_pb = df_pb[\"_pbkt\"].isin([0,1,2])\n",
    "    te_pb = df_pb[\"_pbkt\"].isin([3])\n",
    "\n",
    "    tfidf_pb = TfidfVectorizer(lowercase=True, stop_words=\"english\", min_df=5, max_df=0.9, max_features=5000)\n",
    "    X_pb_tr = tfidf_pb.fit_transform(df_pb.loc[tr_pb, \"text\"].astype(str))\n",
    "    y_pb_tr = df_pb.loc[tr_pb, \"y\"].values\n",
    "    X_pb_te = tfidf_pb.transform(df_pb.loc[te_pb, \"text\"].astype(str))\n",
    "    y_pb_te = df_pb.loc[te_pb, \"y\"].values\n",
    "\n",
    "    clf_pb = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "    clf_pb.fit(X_pb_tr, y_pb_tr)\n",
    "\n",
    "    if len(y_pb_te):\n",
    "        pr_pb = clf_pb.predict_proba(X_pb_te)[:,1]\n",
    "        yhat_pb = (pr_pb >= 0.5).astype(int)\n",
    "        prompt_shift_metrics = {\n",
    "            \"accuracy\": float(accuracy_score(y_pb_te, yhat_pb)),\n",
    "            \"f1\": float(f1_score(y_pb_te, yhat_pb)),\n",
    "            \"roc_auc\": float(roc_auc_score(y_pb_te, pr_pb)),\n",
    "            \"n\": int(len(y_pb_te))\n",
    "        }\n",
    "    else:\n",
    "        prompt_shift_metrics = {\"note\": \"No held-out prompt-bucket rows.\"}\n",
    "else:\n",
    "    prompt_shift_metrics = {\"note\": \"No prompt column available; skipped.\"}\n",
    "\n",
    "prompt_shift_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03b2b6",
   "metadata": {},
   "source": [
    "### 9) Save Plots & Markdown Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_top_words(pairs, title, fname):\n",
    "    words = [w for w,_ in pairs]\n",
    "    vals = [float(v) for _,v in pairs]\n",
    "    y = np.arange(len(words))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.barh(y, vals)\n",
    "    plt.yticks(y, words)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"logistic coef (higher => 'High')\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plot_top_words(top_pos_words[:15], \"Top words associated with HIGH scores\", f\"{ART_DIR}/top_pos_words.png\")\n",
    "plot_top_words(top_neg_words[:15], \"Top words associated with LOW scores\",  f\"{ART_DIR}/top_neg_words.png\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(gaming_df[\"delta\"], bins=10)\n",
    "plt.title(\"Predicted High-Score Probability Increase After Gaming (low texts)\")\n",
    "plt.xlabel(\"Δ probability (gamed - original)\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART_DIR}/gaming_delta_hist.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "ARTIFACT = f\"{ART_DIR}/README_audit_results.md\"\n",
    "with open(ARTIFACT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Reformed-Audit Demo Results (Split-Cells, with ROW_CAP)\\n\\n\")\n",
    "    f.write(\"## Baseline (surface features)\\n\")\n",
    "    f.write(json.dumps(baseline_metrics, indent=2) + \"\\n\\n\")\n",
    "    f.write(\"## Gaming Demo (lowest-probability samples)\\n\")\n",
    "    f.write(gaming_df.describe().to_markdown() + \"\\n\\n\")\n",
    "    f.write(\"## Fragility Check (strip top-50 words)\\n\")\n",
    "    f.write(json.dumps(fragility_metrics, indent=2) + \"\\n\\n\")\n",
    "    f.write(\"## Distribution Shift by Prompt-Bucket (fresh model)\\n\")\n",
    "    try:\n",
    "        f.write(json.dumps(prompt_shift_metrics, indent=2) + \"\\n\")\n",
    "    except NameError:\n",
    "        f.write(\"{\\\"note\\\": \\\"prompt shift not computed\\\"}\\n\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", f\"{ART_DIR}/top_pos_words.png\")\n",
    "print(\" -\", f\"{ART_DIR}/top_neg_words.png\")\n",
    "print(\" -\", f\"{ART_DIR}/gaming_delta_hist.png\")\n",
    "print(\" -\", ARTIFACT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
