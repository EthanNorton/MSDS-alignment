{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9ebd23",
   "metadata": {},
   "source": [
    "\n",
    "# 05 — Audit Validation & Gaming-Resistant Defenses (from scratch)\n",
    "\n",
    "This notebook rebuilds a **clean pipeline** to:\n",
    "1) Recreate interpretable features (length, richness, POS-ish, readability, prompt–response coupling).  \n",
    "2) Fit a **Ridge** model with **cross‑validation** and summarize coefficients.  \n",
    "3) Run **partial‑dependence‑style** sweeps to see how individual features move predicted score.  \n",
    "4) Do a light **subgroup analysis** by **prompt domain** (policy + math + code + general).  \n",
    "5) Plot **lift curves**: how average score increases with more targets met.  \n",
    "6) Prototype **defensive metrics** that reduce gaming: diminishing returns for length and prompt‑coverage checks.\n",
    "\n",
    "*No external downloads required; spaCy is optional.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cad4f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Could not find C:\\Users\\ethan\\Downloads\\train.csv — place 'train.csv' next to this notebook or define df.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m candidates = [Path(\u001b[33m'\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m'\u001b[39m), Path(\u001b[33m'\u001b[39m\u001b[33m/mnt/data/helpsteer_extracted/train.csv\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m     21\u001b[39m DATA_PATH = \u001b[38;5;28mnext\u001b[39m((p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m candidates \u001b[38;5;28;01mif\u001b[39;00m p.exists()), candidates[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m DATA_PATH.exists(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH.resolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m — place \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m next to this notebook or define df.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m df = pd.read_csv(DATA_PATH)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Could not find C:\\Users\\ethan\\Downloads\\train.csv — place 'train.csv' next to this notebook or define df."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0) Setup & Data\n",
    "import os, re, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# If df is already defined, reuse it; otherwise try to load train.csv\n",
    "if 'df' not in globals():\n",
    "    candidates = [Path('train.csv'), Path('/mnt/data/helpsteer_extracted/train.csv')]\n",
    "    DATA_PATH = next((p for p in candidates if p.exists()), candidates[0])\n",
    "    assert DATA_PATH.exists(), f\"Could not find {DATA_PATH.resolve()} — place 'train.csv' next to this notebook or define df.\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded: {DATA_PATH}\")\n",
    "\n",
    "# Column detection\n",
    "def pick(cols, cands):\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    return None\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "prompt_col   = pick(cols, [\"prompt\",\"instruction\",\"question\",\"query\",\"user_input\"])\n",
    "response_col = pick(cols, [\"response\",\"response_text\",\"answer\",\"assistant_response\",\"model_output\",\"completion\"])\n",
    "if response_col is None:\n",
    "    obj = [c for c in cols if df[c].dtype == object]\n",
    "    response_col = next((c for c in obj if c != prompt_col), None)\n",
    "assert response_col is not None, \"No response-like text column found.\"\n",
    "\n",
    "print({\"prompt\": prompt_col, \"response\": response_col})\n",
    "\n",
    "# Build overall_score if missing from common rating columns\n",
    "if \"overall_score\" not in df.columns:\n",
    "    cands = [c for c in [\"helpfulness\",\"correctness\",\"coherence\",\"complexity\",\"verbosity\",\"quality\",\"score\",\"label\"]\n",
    "             if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    assert len(cands) >= 1, \"overall_score missing and no rating columns found to derive it.\"\n",
    "    df[\"overall_score\"] = df[cands].astype(float).mean(axis=1)\n",
    "\n",
    "# Score bins for summaries\n",
    "if \"overall_bin\" not in df.columns:\n",
    "    bins   = [0, 4, 8, 12, 16, 20]\n",
    "    labels = [\"0–4\",\"5–8\",\"9–12\",\"13–16\",\"17–20\"]\n",
    "    df[\"overall_bin\"] = pd.cut(df[\"overall_score\"], bins=bins, labels=labels, include_lowest=True, right=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8577df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1) Tokenization, POS-lite, readability, overlap helpers\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "STOPWORDS = set(ENGLISH_STOP_WORDS) | {\n",
    "    \"im\",\"ive\",\"id\",\"youre\",\"youll\",\"dont\",\"cant\",\"wont\",\"didnt\",\"doesnt\",\"isnt\",\"arent\",\"wasnt\",\"werent\",\n",
    "    \"couldnt\",\"shouldnt\",\"wouldnt\",\"thats\",\"theres\",\"heres\",\"whats\",\"lets\",\n",
    "    \"ok\",\"okay\",\"yes\",\"yeah\",\"nope\",\"uh\",\"um\",\"uhh\",\"hmm\",\n",
    "    \"like\",\"just\",\"really\",\"actually\",\"basically\",\"literally\",\n",
    "    \"etc\",\"e.g\",\"eg\",\"i.e\",\"ie\",\"http\",\"https\",\"www\",\"com\",\"net\",\"org\"\n",
    "}\n",
    "\n",
    "def tokens(text):\n",
    "    return re.findall(r\"[A-Za-z]{2,}\", str(text).lower())\n",
    "\n",
    "VERB_SUFFIXES = (\"ing\",\"ed\",\"en\",\"ize\",\"ise\",\"ify\")\n",
    "ADJ_SUFFIXES  = (\"ous\",\"ful\",\"ive\",\"less\",\"able\",\"ible\",\"al\",\"ary\",\"ic\",\"ical\",\"y\",\"ish\")\n",
    "NOUN_SUFFIXES = (\"tion\",\"sion\",\"ment\",\"ness\",\"ity\",\"ship\",\"ism\",\"ist\",\"ance\",\"ence\",\"ery\",\"or\",\"er\")\n",
    "def is_verbish(t): return t.endswith(VERB_SUFFIXES)\n",
    "def is_adjish(t):  return t.endswith(ADJ_SUFFIXES)\n",
    "def is_nounish(t): return t.endswith(NOUN_SUFFIXES)\n",
    "\n",
    "def syllable_count(word):\n",
    "    w = word.lower()\n",
    "    vowels = \"aeiouy\"; count = 0; prev = False\n",
    "    for ch in w:\n",
    "        v = ch in vowels\n",
    "        if v and not prev: count += 1\n",
    "        prev = v\n",
    "    if w.endswith(\"e\") and count > 1: count -= 1\n",
    "    return max(1, count)\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    toks = tokens(text); \n",
    "    if not toks: return 0.0\n",
    "    n_words = len(toks)\n",
    "    n_sents = max(1, len(re.findall(r\"[.!?]+\", str(text))))\n",
    "    n_syll  = sum(syllable_count(w) for w in toks)\n",
    "    return 206.835 - 1.015*(n_words/n_sents) - 84.6*(n_syll/n_words)\n",
    "\n",
    "def avg_word_len(text):\n",
    "    toks = tokens(text)\n",
    "    return (sum(len(t) for t in toks)/len(toks)) if toks else 0.0\n",
    "\n",
    "def jaccard_overlap(a_text, b_text):\n",
    "    A = set([t for t in tokens(a_text) if t not in STOPWORDS])\n",
    "    B = set([t for t in tokens(b_text) if t not in STOPWORDS])\n",
    "    if not A and not B: return 0.0\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "def pos_counts(text):\n",
    "    toks = tokens(text)\n",
    "    v = sum(1 for t in toks if is_verbish(t))\n",
    "    a = sum(1 for t in toks if is_adjish(t))\n",
    "    n = sum(1 for t in toks if is_nounish(t))\n",
    "    return len(toks), v, a, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2) Feature extraction (response + prompt + relational)\n",
    "RESP = response_col\n",
    "PR   = prompt_col  # may be None\n",
    "\n",
    "def extract_all(text):\n",
    "    toks = tokens(text); n_tok = len(toks)\n",
    "    n_sent = max(1, len(re.findall(r\"[.!?]+\", str(text))))\n",
    "    total, v, a, n = pos_counts(text)\n",
    "    return {\n",
    "        \"resp_word_len\": n_tok,\n",
    "        \"resp_avg_word_len\": avg_word_len(text),\n",
    "        \"resp_tokens_per_sentence\": (n_tok/n_sent) if n_sent else 0.0,\n",
    "        \"resp_type_token_ratio\": (len(set(toks))/n_tok) if n_tok else 0.0,\n",
    "        \"resp_flesch_readability\": flesch_reading_ease(text),\n",
    "        \"resp_adj_count\": a,\n",
    "        \"resp_verb_ratio\": (v/n_tok) if n_tok else 0.0,\n",
    "        \"resp_adj_ratio\": (a/n_tok) if n_tok else 0.0,\n",
    "        \"resp_noun_count\": n,\n",
    "        \"resp_noun_ratio\": (n/n_tok) if n_tok else 0.0,\n",
    "        \"resp_punct_density\": (len(re.findall(r\"[,:;—-]\", str(text))) / max(1, len(str(text)))),\n",
    "    }\n",
    "\n",
    "resp_feat = df[RESP].astype(str).apply(extract_all).apply(pd.Series)\n",
    "\n",
    "if PR is not None and PR in df.columns:\n",
    "    prm_feat_raw = df[PR].astype(str).apply(extract_all).apply(pd.Series)\n",
    "    # lexical density from prompt\n",
    "    def lex_density_prompt(s):\n",
    "        toks = tokens(s)\n",
    "        if not toks: return 0.0\n",
    "        stop = sum(1 for t in toks if t in STOPWORDS)\n",
    "        return (len(toks)-stop)/len(toks)\n",
    "    prm_feat = pd.DataFrame({\n",
    "        \"pr_word_len\":        prm_feat_raw[\"resp_word_len\"],\n",
    "        \"pr_lexical_density\": df[PR].astype(str).apply(lex_density_prompt),\n",
    "        \"pr_hapax_ratio\":     df[PR].astype(str).apply(lambda s: (lambda t: (sum(1 for w in set(t) if t.count(w)==1)/len(t) if len(t)>0 else 0.0))(tokens(s))),\n",
    "        \"pr_punct_density\":   df[PR].astype(str).apply(lambda s: len(re.findall(r\"[,:;—-]\", str(s))) / max(1, len(str(s)))),\n",
    "    })\n",
    "else:\n",
    "    prm_feat = pd.DataFrame(index=df.index, columns=[\"pr_word_len\",\"pr_lexical_density\",\"pr_hapax_ratio\",\"pr_punct_density\"]).fillna(0.0)\n",
    "\n",
    "# Relational\n",
    "if PR is not None and PR in df.columns:\n",
    "    pr_lens = df[PR].astype(str).apply(lambda s: len(tokens(s))).replace(0, np.nan)\n",
    "    rel = pd.DataFrame({\n",
    "        \"prr_len_ratio\": (resp_feat[\"resp_word_len\"] / pr_lens).fillna(0.0),\n",
    "        \"prr_avg_wordlen_diff\": resp_feat[\"resp_avg_word_len\"] - df[PR].astype(str).apply(avg_word_len),\n",
    "        \"prr_overlap_jaccard\": [jaccard_overlap(p, r) for p, r in zip(df[PR].astype(str), df[RESP].astype(str))],\n",
    "    })\n",
    "else:\n",
    "    rel = pd.DataFrame({\"prr_len_ratio\": 0.0, \"prr_avg_wordlen_diff\": resp_feat[\"resp_avg_word_len\"], \"prr_overlap_jaccard\": 0.0}, index=df.index)\n",
    "\n",
    "features = pd.concat([resp_feat, prm_feat, rel], axis=1)\n",
    "data = pd.concat([df.reset_index(drop=True), features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Engineered feature columns:\", len(features.columns))\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b94257",
   "metadata": {},
   "source": [
    "## Cross‑validation: Ridge performance & stable coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3) KFold CV on Ridge\n",
    "X = features.astype(float).fillna(0.0).values\n",
    "y = data[\"overall_score\"].astype(float).values\n",
    "feat_names = list(features.columns)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "r2s, rmses, coefs = [], [], []\n",
    "\n",
    "for tr, te in kf.split(X):\n",
    "    model = make_pipeline(StandardScaler(with_mean=True, with_std=True), Ridge(alpha=2.0, random_state=SEED))\n",
    "    model.fit(X[tr], y[tr])\n",
    "    pred = model.predict(X[te])\n",
    "    r2s.append(r2_score(y[te], pred))\n",
    "    rmses.append(np.sqrt(mean_squared_error(y[te], pred)))\n",
    "    coefs.append(model.named_steps[\"ridge\"].coef_)\n",
    "\n",
    "print(\"CV R^2  (mean ± sd):\", f\"{np.mean(r2s):.3f} ± {np.std(r2s):.3f}\")\n",
    "print(\"CV RMSE (mean ± sd):\", f\"{np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "\n",
    "coef_mean = np.mean(np.stack(coefs, axis=0), axis=0)\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coef_mean}).sort_values(\"coef\", ascending=False)\n",
    "print(\"\\nTop + coefficients (mean across folds):\")\n",
    "display(coef_df.head(15))\n",
    "print(\"Top - coefficients (mean across folds):\")\n",
    "display(coef_df.tail(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163a36a",
   "metadata": {},
   "source": [
    "## Partial‑dependence‑style sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a19801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4) Sweep one feature across percentiles while holding others at dataset mean\n",
    "model_full = make_pipeline(StandardScaler(with_mean=True, with_std=True), Ridge(alpha=2.0, random_state=SEED))\n",
    "model_full.fit(X, y)\n",
    "\n",
    "def partial_dependence_like(feature_name, n_points=30):\n",
    "    assert feature_name in features.columns, f\"{feature_name} not found.\"\n",
    "    X_ref = features.astype(float).fillna(0.0).mean(axis=0).values.reshape(1, -1)\n",
    "    idx = feat_names.index(feature_name)\n",
    "    vals = np.quantile(features[feature_name].astype(float), np.linspace(0.01, 0.99, n_points))\n",
    "    preds = []\n",
    "    for v in vals:\n",
    "        X_tmp = X_ref.copy()\n",
    "        X_tmp[0, idx] = v\n",
    "        preds.append(model_full.predict(X_tmp)[0])\n",
    "    return vals, np.array(preds)\n",
    "\n",
    "to_sweep = coef_df.head(6)[\"feature\"].tolist()\n",
    "plt.figure(figsize=(10,6))\n",
    "for f in to_sweep:\n",
    "    xs, ys = partial_dependence_like(f)\n",
    "    plt.plot(xs, ys, label=f)\n",
    "plt.legend()\n",
    "plt.title(\"Partial‑dependence‑style curves (one feature at a time)\")\n",
    "plt.xlabel(\"feature value\"); plt.ylabel(\"predicted score\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7fffc",
   "metadata": {},
   "source": [
    "## Subgroup analysis by prompt domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5) Heuristic domain tags using keywords in prompt (fallbacks to 'general')\n",
    "def tag_domain(p):\n",
    "    p = str(p).lower()\n",
    "    if any(k in p for k in [\"election\",\"policy\",\"govern\",\"vote\",\"senate\",\"congress\",\"law\"]): return \"policy\"\n",
    "    if any(k in p for k in [\"math\",\"algebra\",\"equation\",\"solve\",\"probability\",\"integral\",\"derivative\"]): return \"math\"\n",
    "    if any(k in p for k in [\"code\",\"python\",\"function\",\"bug\",\"error\",\"compile\",\"algorithm\"]): return \"code\"\n",
    "    return \"general\"\n",
    "\n",
    "domains = df[prompt_col].apply(tag_domain) if prompt_col else pd.Series([\"general\"]*len(df))\n",
    "data[\"domain\"] = domains\n",
    "\n",
    "for dom in [\"policy\",\"math\",\"code\",\"general\"]:\n",
    "    m = (data[\"domain\"] == dom).values\n",
    "    if m.sum() < 50: \n",
    "        print(f\"Skip domain {dom} (n={m.sum()})\")\n",
    "        continue\n",
    "    Xm = features[m].astype(float).fillna(0.0).values\n",
    "    ym = data.loc[m, \"overall_score\"].astype(float).values\n",
    "    model = make_pipeline(StandardScaler(with_mean=True, with_std=True), Ridge(alpha=2.0, random_state=SEED))\n",
    "    model.fit(Xm, ym)\n",
    "    r2 = r2_score(ym, model.predict(Xm))\n",
    "    coef_dom = pd.DataFrame({\"feature\": feat_names, \"coef\": model.named_steps[\"ridge\"].coef_}).sort_values(\"coef\", ascending=False).head(8)\n",
    "    print(f\"\\nDomain: {dom} | n={m.sum()} | in-sample R^2={r2:.3f}\")\n",
    "    display(coef_dom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e8942",
   "metadata": {},
   "source": [
    "## Lift curves: average score vs. # of targets met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6) Build μ + kσ targets for top positive features and plot lift by #hits\n",
    "TOPK = 10; K_STD = 1.0\n",
    "top_pos = coef_df.head(TOPK)[\"feature\"].tolist()\n",
    "\n",
    "def targets_for(df_feat, feat_list, k=K_STD):\n",
    "    rows = []\n",
    "    for f in feat_list:\n",
    "        s = pd.to_numeric(df_feat[f], errors=\"coerce\")\n",
    "        mu, sd = s.mean(), s.std(ddof=0)\n",
    "        rows.append((f, mu + k*sd))\n",
    "    return dict(rows)\n",
    "\n",
    "targets = targets_for(features, top_pos, K_STD)\n",
    "\n",
    "M = []\n",
    "for f, t in targets.items():\n",
    "    M.append((pd.to_numeric(features[f], errors=\"coerce\") >= t).values)\n",
    "M = np.vstack(M).T  # (n_rows, n_features)\n",
    "hits = M.sum(axis=1)\n",
    "\n",
    "tmp = pd.DataFrame({\"hits\": hits, \"score\": data[\"overall_score\"].astype(float)})\n",
    "lift = tmp.groupby(\"hits\")[\"score\"].mean()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(lift.index, lift.values, marker=\"o\")\n",
    "plt.title(f\"Mean score vs #targets met (top {TOPK}, μ+{K_STD}σ)\")\n",
    "plt.xlabel(\"# of targets met\"); plt.ylabel(\"mean overall_score\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3edfc",
   "metadata": {},
   "source": [
    "## Gaming‑resistant defenses (prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b624135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 7) Propose a more robust audit score that reduces gaming\n",
    "# (A) Diminishing returns on length using a log transform\n",
    "length = features[\"resp_word_len\"].astype(float).clip(lower=1.0)\n",
    "len_diminish = np.log1p(length)  # concave\n",
    "\n",
    "# (B) Prompt coverage: combine Jaccard overlap + coverage of rare prompt words\n",
    "from collections import Counter\n",
    "def rare_weighted_overlap(prompt, response):\n",
    "    p = [t for t in tokens(prompt) if t not in STOPWORDS]\n",
    "    r = set([t for t in tokens(response) if t not in STOPWORDS])\n",
    "    if not p: return 0.0\n",
    "    counts = Counter(p)\n",
    "    total = 0.0\n",
    "    hit = 0.0\n",
    "    for w,c in counts.items():\n",
    "        total += 1.0 / c\n",
    "        if w in r: hit += 1.0 / c\n",
    "    return hit / total if total>0 else 0.0\n",
    "\n",
    "if prompt_col:\n",
    "    rw_overlap = [rare_weighted_overlap(p, r) for p, r in zip(df[prompt_col].astype(str), df[response_col].astype(str))]\n",
    "else:\n",
    "    rw_overlap = [0.0]*len(df)\n",
    "\n",
    "# (C) Clarity guardrail for extreme sentence length\n",
    "tokens_per_sent = features[\"resp_tokens_per_sentence\"].astype(float)\n",
    "clarity_bonus = np.exp(-np.maximum(0, tokens_per_sent - tokens_per_sent.median()) / (tokens_per_sent.std(ddof=0)+1e-6))\n",
    "\n",
    "# Combine into a prototype \"robust_score\" (normalize components first)\n",
    "def z(x): \n",
    "    x = np.asarray(x, float)\n",
    "    return (x - np.nanmean(x)) / (np.nanstd(x)+1e-9)\n",
    "\n",
    "robust_score = z(len_diminish) + z(features[\"prr_overlap_jaccard\"]) + z(rw_overlap) + z(features[\"resp_type_token_ratio\"]) + z(features[\"resp_flesch_readability\"]) + z(clarity_bonus)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"overall_score\": data[\"overall_score\"].astype(float),\n",
    "    \"robust_score\": robust_score,\n",
    "    \"resp_word_len\": features[\"resp_word_len\"],\n",
    "    \"prr_overlap_jaccard\": features[\"prr_overlap_jaccard\"]\n",
    "})\n",
    "print(\"Correlation (overall vs robust):\", np.corrcoef(out[\"overall_score\"], out[\"robust_score\"])[0,1].round(3))\n",
    "out.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
