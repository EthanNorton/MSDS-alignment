{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c915c783",
   "metadata": {},
   "source": [
    "\n",
    "# 04 — Deep Language Metrics: Verbs, Adjectives, Readability, and Prompt–Response Dynamics\n",
    "\n",
    "This notebook **extends your current analysis** with richer linguistic and structural features to explain variation in `overall_score`.\n",
    "\n",
    "**Highlights**\n",
    "- Works with your existing `df` (or loads `train.csv` as a fallback).\n",
    "- Extracts **verbs/adjectives/nouns** (spaCy if available, else regex heuristics).\n",
    "- Adds **readability** (Flesch) and composition features (hapax ratio, lexical density).\n",
    "- Compares **prompt vs. response**: length ratios, average word length, and **Jaccard token overlap**.\n",
    "- Computes **hedges/politeness** counts and **punctuation density**.\n",
    "- Produces **correlations**, **bin summaries**, and a quick **Ridge regression** feature ranking.\n",
    "\n",
    "> If `overall_score` is missing, we build a proxy by averaging typical rating columns (helpfulness, correctness, coherence, complexity, verbosity) when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 0) Setup & Data ---\n",
    "import os, re, math, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# Reuse df if present, else try to load train.csv\n",
    "if 'df' not in globals():\n",
    "    candidates = [Path('train.csv'), Path('/mnt/data/helpsteer_extracted/train.csv')]\n",
    "    DATA_PATH = next((p for p in candidates if p.exists()), candidates[0])\n",
    "    assert DATA_PATH.exists(), f\"Could not find {DATA_PATH.resolve()} — put 'train.csv' next to this notebook or provide df.\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f'Loaded: {DATA_PATH}')\n",
    "\n",
    "# Detect columns\n",
    "def pick(cols, cands):\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    return None\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "prompt_col   = pick(cols, [\"prompt\",\"instruction\",\"question\",\"query\",\"user_input\"])\n",
    "response_col = pick(cols, [\"response\",\"response_text\",\"answer\",\"assistant_response\",\"model_output\",\"completion\"])\n",
    "if response_col is None:\n",
    "    # fallback to any object col not equal to prompt\n",
    "    obj = [c for c in cols if df[c].dtype == object]\n",
    "    response_col = next((c for c in obj if c != prompt_col), None)\n",
    "assert response_col is not None, \"No response-like text column found.\"\n",
    "\n",
    "print({\"prompt\": prompt_col, \"response\": response_col})\n",
    "\n",
    "# Build overall_score if missing: mean of common rating cols\n",
    "if \"overall_score\" not in df.columns:\n",
    "    cand_scores = [c for c in [\"helpfulness\",\"correctness\",\"coherence\",\"complexity\",\"verbosity\",\"quality\",\"score\",\"label\"]\n",
    "                   if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    assert len(cand_scores) >= 1, \"overall_score missing and no standard rating columns found to derive it.\"\n",
    "    df[\"overall_score\"] = df[cand_scores].astype(float).mean(axis=1)\n",
    "\n",
    "# Build bins if missing\n",
    "if \"overall_bin\" not in df.columns:\n",
    "    bins   = [0, 4, 8, 12, 16, 20]\n",
    "    labels = [\"0–4\",\"5–8\",\"9–12\",\"13–16\",\"17–20\"]\n",
    "    df[\"overall_bin\"] = pd.cut(df[\"overall_score\"], bins=bins, labels=labels, include_lowest=True, right=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d28dfc",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization and POS (verbs, adjectives, nouns)\n",
    "- If `spaCy` with an English model is available, we use it to count **NOUN/PROPN/ADJ/VERB** precisely.\n",
    "- Otherwise, we fall back to regex-based heuristics for a light-weight approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_SPACY = True\n",
    "spacy_nlp = None\n",
    "if USE_SPACY:\n",
    "    try:\n",
    "        import spacy\n",
    "        for name in [\"en_core_web_sm\",\"en_core_web_md\",\"en_core_web_lg\",\"en_core_web_trf\"]:\n",
    "            try:\n",
    "                spacy_nlp = spacy.load(name, disable=[\"ner\",\"textcat\"])\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if spacy_nlp is None:\n",
    "            raise RuntimeError(\"No local spaCy English model found\")\n",
    "        print(\"spaCy POS tagging enabled:\", spacy_nlp.meta.get(\"name\",\"<unknown>\"))\n",
    "    except Exception as e:\n",
    "        spacy_nlp = None\n",
    "        print(\"spaCy not available; using regex heuristics. (\", e, \")\")\n",
    "\n",
    "STOPWORDS = set(ENGLISH_STOP_WORDS) | {\n",
    "    \"im\",\"ive\",\"id\",\"youre\",\"youll\",\"dont\",\"cant\",\"wont\",\"didnt\",\"doesnt\",\"isnt\",\"arent\",\"wasnt\",\"werent\",\n",
    "    \"couldnt\",\"shouldnt\",\"wouldnt\",\"thats\",\"theres\",\"heres\",\"whats\",\"lets\",\n",
    "    \"ok\",\"okay\",\"yes\",\"yeah\",\"nope\",\"uh\",\"um\",\"uhh\",\"hmm\",\n",
    "    \"like\",\"just\",\"really\",\"actually\",\"basically\",\"literally\",\n",
    "    \"etc\",\"e.g\",\"eg\",\"i.e\",\"ie\",\n",
    "    \"http\",\"https\",\"www\",\"com\",\"net\",\"org\"\n",
    "}\n",
    "\n",
    "# Simple regex tokenizer\n",
    "def tokens(text):\n",
    "    return re.findall(r\"[A-Za-z]{2,}\", str(text).lower())\n",
    "\n",
    "# Crude verb/adj/noun heuristics if no spaCy\n",
    "VERB_SUFFIXES = (\"ing\",\"ed\",\"en\",\"ize\",\"ise\",\"ify\")\n",
    "ADJ_SUFFIXES  = (\"ous\",\"ful\",\"ive\",\"less\",\"able\",\"ible\",\"al\",\"ary\",\"ic\",\"ical\",\"y\",\"ish\")\n",
    "NOUN_SUFFIXES = (\"tion\",\"sion\",\"ment\",\"ness\",\"ity\",\"ship\",\"ism\",\"ist\",\"ance\",\"ence\",\"ery\",\"or\",\"er\")\n",
    "\n",
    "def is_verbish(tok): return tok.endswith(VERB_SUFFIXES)\n",
    "def is_adjish(tok):  return tok.endswith(ADJ_SUFFIXES)\n",
    "def is_nounish(tok): return tok.endswith(NOUN_SUFFIXES)\n",
    "\n",
    "def pos_counts(text):\n",
    "    toks = tokens(text)\n",
    "    if spacy_nlp is not None:\n",
    "        doc = spacy_nlp(text or \"\")\n",
    "        v = sum(1 for t in doc if t.pos_==\"VERB\")\n",
    "        a = sum(1 for t in doc if t.pos_==\"ADJ\")\n",
    "        n = sum(1 for t in doc if t.pos_ in {\"NOUN\",\"PROPN\"})\n",
    "        return len(toks), v, a, n\n",
    "    # fallback\n",
    "    v = sum(1 for t in toks if is_verbish(t))\n",
    "    a = sum(1 for t in toks if is_adjish(t))\n",
    "    n = sum(1 for t in toks if is_nounish(t))\n",
    "    return len(toks), v, a, n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e7b1f",
   "metadata": {},
   "source": [
    "\n",
    "## Readability, hedges, politeness, overlap, and other helpers\n",
    "- **Flesch Reading Ease** (no external libs; simple syllable heuristic).\n",
    "- **Hedges** and **politeness markers** (count occurrences).\n",
    "- **Prompt–response** comparisons: lengths, avg word length, and **Jaccard token overlap**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEDGES = {\n",
    "    \"may\",\"might\",\"could\",\"possibly\",\"perhaps\",\"apparently\",\"seems\",\"appear\",\"likely\",\"unlikely\",\"roughly\",\n",
    "    \"approximately\",\"around\",\"about\",\"generally\",\"somewhat\",\"often\",\"sometimes\",\"probably\",\"arguably\"\n",
    "}\n",
    "POLITE = {\"please\",\"thank\",\"thanks\",\"appreciate\",\"kindly\",\"sorry\",\"apologies\"}\n",
    "\n",
    "def count_set(words, vocab):\n",
    "    s = 0\n",
    "    for w in words:\n",
    "        if w in vocab: s += 1\n",
    "    return s\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    if not word: return 0\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0; prev_v = False\n",
    "    for ch in word:\n",
    "        is_v = ch in vowels\n",
    "        if is_v and not prev_v:\n",
    "            count += 1\n",
    "        prev_v = is_v\n",
    "    if word.endswith(\"e\") and count > 1:\n",
    "        count -= 1\n",
    "    return max(1, count)\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    toks = tokens(text)\n",
    "    if not toks: return 0.0\n",
    "    n_words = len(toks)\n",
    "    n_sents = max(1, len(re.findall(r\"[.!?]+\", str(text))))\n",
    "    n_syll  = sum(syllable_count(w) for w in toks)\n",
    "    return 206.835 - 1.015*(n_words/n_sents) - 84.6*(n_syll/n_words)\n",
    "\n",
    "def lexical_density(text):\n",
    "    toks = tokens(text)\n",
    "    if not toks: return 0.0\n",
    "    content = [t for t in toks if t not in STOPWORDS]\n",
    "    return len(content)/len(toks)\n",
    "\n",
    "def hapax_ratio(text):\n",
    "    toks = tokens(text)\n",
    "    if not toks: return 0.0\n",
    "    from collections import Counter\n",
    "    c = Counter(toks)\n",
    "    hapax = sum(1 for k,v in c.items() if v==1)\n",
    "    return hapax / len(toks)\n",
    "\n",
    "def avg_word_len(text):\n",
    "    toks = tokens(text)\n",
    "    return (sum(len(t) for t in toks)/len(toks)) if toks else 0.0\n",
    "\n",
    "def punct_density(text):\n",
    "    chars = len(str(text))\n",
    "    if chars == 0: return 0.0\n",
    "    n_punct = len(re.findall(r\"[,:;—-]\", str(text)))\n",
    "    return n_punct / chars\n",
    "\n",
    "def jaccard_overlap(a_text, b_text):\n",
    "    A = set([t for t in tokens(a_text) if t not in STOPWORDS])\n",
    "    B = set([t for t in tokens(b_text) if t not in STOPWORDS])\n",
    "    if not A and not B: return 0.0\n",
    "    return len(A & B) / max(1, len(A | B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eedac6",
   "metadata": {},
   "source": [
    "\n",
    "## Feature extraction\n",
    "Build a rich feature set for both **response** and (if available) **prompt**, then aggregate to a modeling table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESP = response_col\n",
    "PR = prompt_col  # may be None\n",
    "\n",
    "def extract_all(text):\n",
    "    toks = tokens(text)\n",
    "    n_tok = len(toks)\n",
    "    n_sent = max(1, len(re.findall(r\"[.!?]+\", str(text))))\n",
    "    n_chars = sum(len(t) for t in toks)\n",
    "    total, v, a, n = pos_counts(text)\n",
    "    hedges = count_set(toks, HEDGES)\n",
    "    polite = count_set(toks, POLITE)\n",
    "    return {\n",
    "        \"word_len\": n_tok,\n",
    "        \"avg_word_len\": avg_word_len(text),\n",
    "        \"sent_count\": n_sent,\n",
    "        \"type_token_ratio\": (len(set(toks))/n_tok) if n_tok else 0.0,\n",
    "        \"lexical_density\": lexical_density(text),\n",
    "        \"hapax_ratio\": hapax_ratio(text),\n",
    "        \"verb_count\": v,\n",
    "        \"adj_count\": a,\n",
    "        \"noun_count\": n,\n",
    "        \"verb_ratio\": (v/n_tok) if n_tok else 0.0,\n",
    "        \"adj_ratio\": (a/n_tok) if n_tok else 0.0,\n",
    "        \"noun_ratio\": (n/n_tok) if n_tok else 0.0,\n",
    "        \"flesch_readability\": flesch_reading_ease(text),\n",
    "        \"hedge_count\": hedges,\n",
    "        \"politeness_count\": polite,\n",
    "        \"punct_density\": punct_density(text),\n",
    "        \"tokens_per_sentence\": (n_tok/n_sent) if n_sent else 0.0,\n",
    "    }\n",
    "\n",
    "resp_feat = df[RESP].astype(str).apply(extract_all).apply(pd.Series)\n",
    "resp_feat.columns = [f\"resp_{c}\" for c in resp_feat.columns]\n",
    "\n",
    "if PR is not None and PR in df.columns:\n",
    "    prm_feat = df[PR].astype(str).apply(extract_all).apply(pd.Series)\n",
    "    prm_feat.columns = [f\"pr_{c}\" for c in prm_feat.columns]\n",
    "else:\n",
    "    prm_feat = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Prompt–Response relational features\n",
    "if PR is not None and PR in df.columns:\n",
    "    overlap = []\n",
    "    for p,r in zip(df[PR].astype(str).tolist(), df[RESP].astype(str).tolist()):\n",
    "        overlap.append(jaccard_overlap(p, r))\n",
    "    rel = pd.DataFrame({\n",
    "        \"prr_len_ratio\": (resp_feat[\"resp_word_len\"] / (prm_feat[\"pr_word_len\"].replace(0, np.nan))).fillna(0.0),\n",
    "        \"prr_avg_wordlen_diff\": resp_feat[\"resp_avg_word_len\"] - prm_feat[\"pr_avg_word_len\"],\n",
    "        \"prr_overlap_jaccard\": overlap,\n",
    "    })\n",
    "else:\n",
    "    rel = pd.DataFrame({\n",
    "        \"prr_len_ratio\": np.zeros(len(df)),\n",
    "        \"prr_avg_wordlen_diff\": np.zeros(len(df)),\n",
    "        \"prr_overlap_jaccard\": np.zeros(len(df)),\n",
    "    })\n",
    "\n",
    "features = pd.concat([resp_feat, prm_feat, rel], axis=1)\n",
    "data = pd.concat([df.reset_index(drop=True), features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Feature columns:\", [c for c in features.columns][:10], \"... (+ more)\")\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16dabc5",
   "metadata": {},
   "source": [
    "\n",
    "## Correlations and summaries\n",
    "We compute Pearson correlations of each feature with `overall_score` and summarize by `overall_bin`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa79a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlations\n",
    "num_cols = [c for c in features.columns]  # all engineered features are numeric\n",
    "corrs = []\n",
    "for c in num_cols:\n",
    "    m = data[c].notna() & data[\"overall_score\"].notna()\n",
    "    if m.sum() >= 10:\n",
    "        r = np.corrcoef(data.loc[m, c].astype(float), data.loc[m, \"overall_score\"].astype(float))[0,1]\n",
    "        corrs.append((c, r))\n",
    "    else:\n",
    "        corrs.append((c, np.nan))\n",
    "corr_df = pd.DataFrame(corrs, columns=[\"feature\",\"pearson_r\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "print(\"Top + correlations:\")\n",
    "display(corr_df.head(20))\n",
    "print(\"Top - correlations:\")\n",
    "display(corr_df.tail(20))\n",
    "\n",
    "# Summaries by bin\n",
    "bin_order = [\"0–4\",\"5–8\",\"9–12\",\"13–16\",\"17–20\"]\n",
    "summary = (data.groupby(\"overall_bin\")[num_cols].agg([\"mean\",\"median\",\"std\"]).reindex(bin_order))\n",
    "summary.round(3).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c1d8f",
   "metadata": {},
   "source": [
    "\n",
    "## Visuals\n",
    "Basic plots for a few key features vs. score. You can add more as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfe451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scatter_with_fit(x, y, title, xlabel, ylabel):\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x0, y0 = x[m], y[m]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x0, y0, s=6, alpha=0.35)\n",
    "    if len(x0) >= 5:\n",
    "        coef = np.polyfit(x0, y0, deg=1)\n",
    "        xs = np.linspace(np.percentile(x0, 1), np.percentile(x0, 99), 200)\n",
    "        ys = coef[0]*xs + coef[1]\n",
    "        plt.plot(xs, ys)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel); plt.tight_layout(); plt.show()\n",
    "\n",
    "scatter_with_fit(data[\"resp_word_len\"].astype(float).values,\n",
    "                 data[\"overall_score\"].astype(float).values,\n",
    "                 \"Response Word Length vs Overall Score\", \"resp_word_len\", \"overall_score\")\n",
    "\n",
    "scatter_with_fit(data[\"resp_adj_ratio\"].astype(float).values,\n",
    "                 data[\"overall_score\"].astype(float).values,\n",
    "                 \"Adjective Ratio vs Overall Score\", \"resp_adj_ratio\", \"overall_score\")\n",
    "\n",
    "scatter_with_fit(data[\"resp_verb_ratio\"].astype(float).values,\n",
    "                 data[\"overall_score\"].astype(float).values,\n",
    "                 \"Verb Ratio vs Overall Score\", \"resp_verb_ratio\", \"overall_score\")\n",
    "\n",
    "scatter_with_fit(data[\"resp_flesch_readability\"].astype(float).values,\n",
    "                 data[\"overall_score\"].astype(float).values,\n",
    "                 \"Flesch Readability vs Overall Score\", \"resp_flesch_readability\", \"overall_score\")\n",
    "\n",
    "if \"prr_overlap_jaccard\" in data.columns:\n",
    "    scatter_with_fit(data[\"prr_overlap_jaccard\"].astype(float).values,\n",
    "                     data[\"overall_score\"].astype(float).values,\n",
    "                     \"Prompt–Response Overlap (Jaccard) vs Overall Score\", \"prr_overlap_jaccard\", \"overall_score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d7d30",
   "metadata": {},
   "source": [
    "\n",
    "## Quick Ridge regression for feature ranking\n",
    "Train a small Ridge model on engineered features to see which signals contribute the most in a linear sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data[[c for c in features.columns]].astype(float).fillna(0.0).values\n",
    "y = data[\"overall_score\"].astype(float).values\n",
    "\n",
    "# Keep only finite rows\n",
    "m = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "X, y = X[m], y[m]\n",
    "\n",
    "model = make_pipeline(StandardScaler(with_mean=True, with_std=True), Ridge(alpha=2.0, random_state=42))\n",
    "model.fit(X, y)\n",
    "y_hat = model.predict(X)\n",
    "print(\"In-sample RMSE:\", mean_squared_error(y, y_hat, squared=False))\n",
    "print(\"In-sample R^2 :\", r2_score(y, y_hat))\n",
    "\n",
    "ridge = model.named_steps[\"ridge\"]\n",
    "coef = ridge.coef_\n",
    "feat_names = [c for c in features.columns]\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coef}).sort_values(\"coef\", ascending=False)\n",
    "print(\"Top positive features (Ridge):\")\n",
    "display(coef_df.head(20))\n",
    "print(\"Top negative features (Ridge):\")\n",
    "display(coef_df.tail(20))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}